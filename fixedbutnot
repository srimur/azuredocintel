import os
import json
import time
import logging
from typing import Dict, List, Any, TypedDict
from dotenv import load_dotenv
from openai.error import RateLimitError, OpenAIError
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langgraph.graph import StateGraph, END

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()

class GraphState(TypedDict):
    alert: Dict[str, Any]
    call_id: str
    user_id: str
    call_docs: List[Dict[str, Any]]
    user_context_docs: List[Dict[str, Any]]
    final_prompt: str
    insights: Dict[str, Any]

def validate_env_variables() -> bool:
    required_vars = [
        "AZURE_SEARCH_ENDPOINT", "AZURE_SEARCH_KEY", "AZURE_SEARCH_INDEX",
        "AZURE_OPENAI_API_BASE", "AZURE_OPENAI_API_KEY",
        "AZURE_OPENAI_DEPLOYMENT", "AZURE_OPENAI_API_VERSION"
    ]
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        logger.error(f"Missing environment variables: {missing}")
        return False
    logger.info("âœ… Environment variables validated")
    return True

def create_azure_search_client() -> SearchClient:
    return SearchClient(
        endpoint=os.getenv("AZURE_SEARCH_ENDPOINT"),
        index_name=os.getenv("AZURE_SEARCH_INDEX"),
        credential=AzureKeyCredential(os.getenv("AZURE_SEARCH_KEY"))
    )

def create_azure_openai_client() -> AzureChatOpenAI:
    return AzureChatOpenAI(
        azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
        openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        azure_endpoint=os.getenv("AZURE_OPENAI_API_BASE"),
        openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        temperature=0, max_tokens=1000, request_timeout=30, max_retries=0
    )

def truncate_data_for_token_limit(data: List[Dict], max_items: int = 3) -> List[Dict]:
    fields = [
        'id', 'timestamp', 'participants_user_displayName', 'organizer_user_displayName',
        'sessions_segments_callerIpSubnet', 'sessions_segments_callerIpAddress'
    ]
    return [{k: (v[:100] + "..." if isinstance(v, str) and len(v) > 100 else v)
             for k, v in d.items() if k in fields}
            for d in data[:max_items]]

def load_alert_node(state: GraphState) -> GraphState:
    try:
        with open("alert.json", 'r') as f:
            alert = json.load(f)
        state["alert"] = alert
        state["call_id"] = alert.get("call_id", "")
        state["user_id"] = alert.get("user_id", "")
    except Exception as e:
        logger.error(f"Error loading alert: {e}")
        state["alert"] = {}
    return state

def fetch_call_node(state: GraphState) -> GraphState:
    try:
        search = create_azure_search_client()
        call_id = state.get("call_id", "")
        if not call_id:
            return state
        results = search.search(
            search_text=f"id:{call_id}",
            top=1,
            select=[
                "id", "timestamp", "participants_user_displayName", "organizer_user_displayName",
                "sessions_segments_callerIpSubnet", "sessions_segments_callerIpAddress"
            ]
        )
        state["call_docs"] = [dict(r) for r in results]
    except Exception as e:
        logger.error(f"Call fetch error: {e}")
        state["call_docs"] = []
    return state

def fetch_user_context_node(state: GraphState) -> GraphState:
    try:
        user_display_name = state.get("user_id", "")
        if not user_display_name:
            return state
        search = create_azure_search_client()
        results = search.search(
            search_text=user_display_name,
            top=3,
            select=[
                "id", "timestamp", "participants_user_displayName", "organizer_user_displayName",
                "sessions_segments_callerIpSubnet", "sessions_segments_callerIpAddress"
            ]
        )
        state["user_context_docs"] = [dict(r) for r in results]
    except Exception as e:
        logger.error(f"User context fetch error: {e}")
        state["user_context_docs"] = []
    return state

def build_prompt_node(state: GraphState) -> GraphState:
    try:
        current = truncate_data_for_token_limit(state.get("call_docs", []), 1)
        history = truncate_data_for_token_limit(state.get("user_context_docs", []), 2)
        state["final_prompt"] = f"""
Analyze Microsoft Teams call quality issue.

Call ID: {state.get("call_id", "unknown")}
User: {state.get("user_id", "unknown")}

Current Call:
{json.dumps(current, indent=1)}

User Context:
{json.dumps(history, indent=1)}

Return JSON:
{{
  "root_cause": "...",
  "recommendations": ["...", "...", "..."],
  "chosen_action": "..."
}}"""
    except Exception as e:
        logger.error(f"Prompt build error: {e}")
        state["final_prompt"] = ""
    return state

def llm_analysis_node(state: GraphState) -> GraphState:
    prompt = state.get("final_prompt", "")
    if not prompt:
        state["insights"] = {"error": "Prompt missing"}
        return state

    llm = create_azure_openai_client()
    retries = 3
    delay = 2
    for attempt in range(retries):
        try:
            response = llm.invoke(prompt)
            content = response.content.strip() if hasattr(response, "content") else str(response).strip()
            if content.startswith("```json"):
                content = content[7:-3]
            elif content.startswith("```"):
                content = content[3:-3]
            state["insights"] = json.loads(content)
            return state
        except RateLimitError:
            logger.warning(f"Rate limit. Retry {attempt+1}")
            time.sleep(delay)
            delay *= 2
        except OpenAIError as e:
            logger.error(f"OpenAI error: {e}")
            state["insights"] = {"error": str(e)}
            return state
        except Exception as e:
            logger.error(f"LLM error: {e}")
            state["insights"] = {"error": str(e)}
            return state
    state["insights"] = {"error": "LLM failed after retries"}
    return state

def print_output_node(state: GraphState) -> GraphState:
    print("\nğŸ” TEAMS CALL QUALITY ANALYSIS\n" + "="*60)
    insights = state.get("insights", {})
    if "error" in insights:
        print(f"âŒ Error: {insights['error']}")
    else:
        print(f"ğŸ¯ Root Cause: {insights.get('root_cause')}")
        print("ğŸ’¡ Recommendations:")
        for i, rec in enumerate(insights.get("recommendations", []), 1):
            print(f"  {i}. {rec}")
        print(f"ğŸš€ Priority Action: {insights.get('chosen_action')}")
    return state

def create_workflow_graph():
    g = StateGraph(GraphState)
    g.add_node("load_alert", load_alert_node)
    g.add_node("fetch_call", fetch_call_node)
    g.add_node("fetch_user_context", fetch_user_context_node)
    g.add_node("build_prompt", build_prompt_node)
    g.add_node("llm_analysis", llm_analysis_node)
    g.add_node("print_output", print_output_node)
    g.set_entry_point("load_alert")
    g.add_edge("load_alert", "fetch_call")
    g.add_edge("fetch_call", "fetch_user_context")
    g.add_edge("fetch_user_context", "build_prompt")
    g.add_edge("build_prompt", "llm_analysis")
    g.add_edge("llm_analysis", "print_output")
    g.add_edge("print_output", END)
    return g

def main():
    print("ğŸš€ Starting Teams Call Quality Analyzer")
    if not validate_env_variables():
        return
    if not os.path.exists("alert.json"):
        with open("alert.json", "w") as f:
            json.dump({"call_id": "REPLACE_ME", "user_id": "REPLACE_ME"}, f)
    graph = create_workflow_graph()
    app = graph.compile()
    state = {
        "alert": {},
        "call_id": "",
        "user_id": "",
        "call_docs": [],
        "user_context_docs": [],
        "final_prompt": "",
        "insights": {}
    }
    app.invoke(state)

if __name__ == "__main__":
    main()
