"""
Microsoft Teams Call Quality Analysis System
This system analyzes poor quality Teams calls and provides remediation recommendations.
"""

import os
import json
import logging
import time
import random
from typing import Dict, List, Any, TypedDict
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.core.exceptions import AzureError
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_community.vectorstores.azuresearch import AzureSearch
from langgraph.graph import StateGraph, END

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class GraphState(TypedDict):
    """Type definition for the graph state"""
    alert: Dict[str, Any]
    call_id: str
    user_id: str
    call_docs: List[Dict[str, Any]]
    user_context_docs: List[Dict[str, Any]]
    final_prompt: str
    insights: Dict[str, Any]

def retry_with_exponential_backoff(
    func,
    max_retries: int = 3,
    initial_delay: float = 1.0,
    backoff_factor: float = 2.0,
    max_delay: float = 60.0
):
    """Retry function with exponential backoff for rate limiting"""
    def wrapper(*args, **kwargs):
        delay = initial_delay
        for attempt in range(max_retries + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                error_msg = str(e).lower()
                
                # Check if it's a rate limit error
                if any(keyword in error_msg for keyword in ['rate limit', 'quota', 'throttle', '429']):
                    if attempt < max_retries:
                        # Add jitter to prevent thundering herd
                        jitter = random.uniform(0, 0.1) * delay
                        sleep_time = min(delay + jitter, max_delay)
                        
                        logger.warning(f"Rate limit hit, retrying in {sleep_time:.2f}s (attempt {attempt + 1}/{max_retries + 1})")
                        time.sleep(sleep_time)
                        delay *= backoff_factor
                    else:
                        logger.error(f"Rate limit exceeded after {max_retries} retries")
                        raise
                else:
                    # Non-rate limit error, don't retry
                    raise
        
        return func(*args, **kwargs)
    return wrapper

def truncate_data_for_token_limit(data: List[Dict], max_items: int = 3) -> List[Dict]:
    """Truncate data to stay within token limits"""
    if not data:
        return []
    
    # Take only the most recent/relevant items
    truncated = data[:max_items]
    
    # Further truncate each item to essential fields only
    essential_fields = [
        'id', 'user_id', 'timestamp', 'quality_metrics', 
        'network_metrics', 'audio_quality', 'video_quality',
        'packet_loss', 'latency', 'jitter', 'displayName',
        'participants_user_displayName', 'organizer_user_displayName'
    ]
    
    simplified_data = []
    for item in truncated:
        simplified_item = {}
        for key, value in item.items():
            if key in essential_fields:
                # Truncate long string values
                if isinstance(value, str) and len(value) > 100:
                    simplified_item[key] = value[:100] + "..."
                else:
                    simplified_item[key] = value
        simplified_data.append(simplified_item)
    
    return simplified_data

def validate_env_variables() -> bool:
    """Validate all required environment variables are present"""
    required_vars = [
        "AZURE_SEARCH_ENDPOINT",
        "AZURE_SEARCH_KEY", 
        "AZURE_SEARCH_INDEX",
        "AZURE_OPENAI_API_BASE",
        "AZURE_OPENAI_API_KEY",
        "AZURE_OPENAI_DEPLOYMENT",
        "AZURE_OPENAI_API_VERSION"
    ]
    
    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        logger.error(f"Missing required environment variables: {missing_vars}")
        return False
    
    logger.info("‚úÖ All environment variables validated")
    return True

def create_azure_search_client() -> SearchClient:
    """Create Azure Search client with proper error handling"""
    try:
        search_credential = AzureKeyCredential(os.getenv("AZURE_SEARCH_KEY"))
        client = SearchClient(
            endpoint=os.getenv("AZURE_SEARCH_ENDPOINT"),
            index_name=os.getenv("AZURE_SEARCH_INDEX"),
            credential=search_credential
        )
        logger.info("‚úÖ Azure Search client created successfully")
        return client
    except Exception as e:
        logger.error(f"‚ùå Failed to create Azure Search client: {e}")
        raise

def create_azure_openai_client() -> AzureChatOpenAI:
    """Create Azure OpenAI client with S0 tier optimizations"""
    try:
        client = AzureChatOpenAI(
            azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_API_BASE"),
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            temperature=0,
            max_tokens=1000,  # Limit tokens for S0 tier
            request_timeout=30,  # Add timeout
            max_retries=2  # Limit retries
        )
        logger.info("‚úÖ Azure OpenAI client created successfully (S0 optimized)")
        return client
    except Exception as e:
        logger.error(f"‚ùå Failed to create Azure OpenAI client: {e}")
        raise

def create_embeddings_client() -> AzureOpenAIEmbeddings:
    """Create Azure OpenAI embeddings client"""
    try:
        client = AzureOpenAIEmbeddings(
            azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_API_BASE"),
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY")
        )
        logger.info("‚úÖ Azure OpenAI embeddings client created successfully")
        return client
    except Exception as e:
        logger.error(f"‚ùå Failed to create embeddings client: {e}")
        raise

# -------- LangGraph Node Functions -------- #

def load_alert_node(state: GraphState) -> GraphState:
    """Load alert data from JSON file"""
    logger.info("üì• Loading alert data...")
    
    try:
        alert_file = "alert.json"
        if not os.path.exists(alert_file):
            logger.error(f"‚ùå Alert file '{alert_file}' not found")
            state["alert"] = {}
            state["call_id"] = ""
            state["user_id"] = ""
            return state
        
        with open(alert_file, 'r', encoding='utf-8') as f:
            alert = json.load(f)
        
        state["alert"] = alert
        state["call_id"] = alert.get("call_id", "")
        state["user_id"] = alert.get("user_id", "")
        
        logger.info(f"‚úÖ Alert loaded - Call ID: {state['call_id']}, User: {state['user_id']}")
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Invalid JSON in alert file: {e}")
        state["alert"] = {}
        state["call_id"] = ""
        state["user_id"] = ""
    except Exception as e:
        logger.error(f"‚ùå Error loading alert: {e}")
        state["alert"] = {}
        state["call_id"] = ""
        state["user_id"] = ""
    
    return state

def fetch_call_node(state: GraphState) -> GraphState:
    """Fetch call documents from Azure Search"""
    logger.info("üîç Fetching call documents...")
    
    try:
        call_id = state.get("call_id", "")
        if not call_id:
            logger.warning("‚ö†Ô∏è No call_id available")
            state["call_docs"] = []
            return state
        
        search_client = create_azure_search_client()
        # Use 'id' field from Azure Search index, not 'call_id'
        search_query = f"id:{call_id}"
        
        results = search_client.search(
            search_text=search_query,
            top=5,  # Reduced from 10 to limit tokens
            select=["id", "timestamp", "quality_metrics", "network_metrics", "participants_user_displayName", "organizer_user_displayName"]
        )
        
        # Convert search results to serializable format
        call_docs = []
        for doc in results:
            if isinstance(doc, dict):
                call_docs.append(doc)
            else:
                # Convert search result object to dict
                doc_dict = {}
                for key in doc.keys():
                    doc_dict[key] = doc[key]
                call_docs.append(doc_dict)
        
        state["call_docs"] = call_docs
        logger.info(f"‚úÖ Found {len(call_docs)} call documents")
        
    except AzureError as e:
        logger.error(f"‚ùå Azure Search error: {e}")
        state["call_docs"] = []
    except Exception as e:
        logger.error(f"‚ùå Error fetching call data: {e}")
        state["call_docs"] = []
    
    return state

def fetch_user_context_node(state: GraphState) -> GraphState:
    """Fetch user context documents from Azure Search"""
    logger.info("üë§ Fetching user context...")
    
    try:
        user_id = state.get("user_id", "")
        if not user_id:
            logger.warning("‚ö†Ô∏è No user_id available")
            state["user_context_docs"] = []
            return state
        
        search_client = create_azure_search_client()
        # Search for user in both participant and organizer fields using displayName
        search_query = f"participants_user_displayName:\"{user_id}\" OR organizer_user_displayName:\"{user_id}\""
        
        results = search_client.search(
            search_text=search_query,
            top=3,  # Reduced from 5 to limit tokens
            select=["id", "displayName", "timestamp", "participants_user_displayName", "organizer_user_displayName", "quality_metrics"]
        )
        
        # Convert search results to serializable format
        context_docs = []
        for doc in results:
            if isinstance(doc, dict):
                context_docs.append(doc)
            else:
                # Convert search result object to dict
                doc_dict = {}
                for key in doc.keys():
                    doc_dict[key] = doc[key]
                context_docs.append(doc_dict)
        
        state["user_context_docs"] = context_docs
        logger.info(f"‚úÖ Found {len(context_docs)} user context documents")
        
    except AzureError as e:
        logger.error(f"‚ùå Azure Search error: {e}")
        state["user_context_docs"] = []
    except Exception as e:
        logger.error(f"‚ùå Error fetching user context: {e}")
        state["user_context_docs"] = []
    
    return state

def build_prompt_node(state: GraphState) -> GraphState:
    """Build the analysis prompt for the LLM with token optimization"""
    logger.info("üìù Building analysis prompt...")
    
    try:
        alert = state.get("alert", {})
        current = state.get("call_docs", [])
        history = state.get("user_context_docs", [])
        
        # Truncate data to stay within token limits
        current_truncated = truncate_data_for_token_limit(current, max_items=2)
        history_truncated = truncate_data_for_token_limit(history, max_items=2)
        
        call_id = alert.get("call_id", "unknown")
        user_id = alert.get("user_id", "unknown")
        
        # Optimized prompt for S0 tier - shorter and more focused
        prompt = f"""Analyze Teams call quality issue.

Call ID: {call_id}
User ID: {user_id}

Current Call Data:
{json.dumps(current_truncated, indent=1)}

User History:
{json.dumps(history_truncated, indent=1)}

Provide analysis in JSON format:
{{
  "root_cause": "brief cause",
  "recommendations": ["action1", "action2", "action3"],
  "chosen_action": "most critical action"
}}

Response must be valid JSON only."""

        state["final_prompt"] = prompt
        logger.info("‚úÖ Optimized analysis prompt built")
        
    except Exception as e:
        logger.error(f"‚ùå Error building prompt: {e}")
        state["final_prompt"] = ""
    
    return state

def llm_analysis_node(state: GraphState) -> GraphState:
    """Analyze the data using Azure OpenAI with rate limiting"""
    logger.info("ü§ñ Running LLM analysis...")
    
    try:
        prompt = state.get("final_prompt", "")
        if not prompt:
            logger.error("‚ùå No prompt available for analysis")
            state["insights"] = {"error": "No prompt available"}
            return state
        
        # Add throttling delay for S0 tier
        time.sleep(2)  # 2-second delay to avoid rate limits
        
        llm = create_azure_openai_client()
        
        # Wrap the LLM call with retry logic
        @retry_with_exponential_backoff
        def make_llm_call():
            return llm.invoke(prompt)
        
        response = make_llm_call()
        
        # Extract content from response
        if hasattr(response, 'content'):
            response_text = response.content
        else:
            response_text = str(response)
        
        logger.info("‚úÖ LLM response received")
        
        # Parse JSON response with better error handling
        try:
            # Clean the response text
            response_text = response_text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:-3]
            elif response_text.startswith('```'):
                response_text = response_text[3:-3]
            
            insights = json.loads(response_text)
            
            # Validate the response structure
            if not all(key in insights for key in ['root_cause', 'recommendations', 'chosen_action']):
                logger.warning("‚ö†Ô∏è LLM response missing required fields")
                insights = {
                    "root_cause": insights.get('root_cause', 'Unable to determine'),
                    "recommendations": insights.get('recommendations', ['No recommendations available']),
                    "chosen_action": insights.get('chosen_action', 'No action specified')
                }
            
            state["insights"] = insights
            logger.info("‚úÖ LLM response parsed successfully")
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Failed to parse LLM response as JSON: {e}")
            # Create a fallback response
            state["insights"] = {
                "root_cause": "Unable to parse LLM response",
                "recommendations": [
                    "Check network connectivity",
                    "Verify audio/video devices",
                    "Update Teams client"
                ],
                "chosen_action": "Check network connectivity",
                "error": f"JSON parsing failed: {str(e)}",
                "raw_response": response_text[:200] + "..." if len(response_text) > 200 else response_text
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error in LLM analysis: {e}")
        state["insights"] = {
            "root_cause": "Analysis failed due to system error",
            "recommendations": [
                "Check system logs",
                "Verify Azure connectivity",
                "Contact system administrator"
            ],
            "chosen_action": "Check system logs",
            "error": f"LLM analysis failed: {str(e)}"
        }
    
    return state

def print_output_node(state: GraphState) -> GraphState:
    """Print the final analysis results"""
    logger.info("üìã Generating final output...")
    
    print("\n" + "="*60)
    print("üîç TEAMS CALL QUALITY ANALYSIS RESULTS")
    print("="*60)
    
    insights = state.get("insights", {})
    
    if "error" in insights:
        print(f"‚ùå Analysis Error: {insights['error']}")
        if "raw_response" in insights:
            print(f"Raw Response: {insights['raw_response']}")
    else:
        print(f"üéØ Root Cause: {insights.get('root_cause', 'Not identified')}")
        print(f"\nüí° Recommendations:")
        for i, rec in enumerate(insights.get('recommendations', []), 1):
            print(f"   {i}. {rec}")
        print(f"\nüöÄ Priority Action: {insights.get('chosen_action', 'Not specified')}")
    
    print("="*60)
    print(f"üìä Analysis Summary:")
    print(f"   ‚Ä¢ Alert processed: {bool(state.get('alert'))}")
    print(f"   ‚Ä¢ Call documents: {len(state.get('call_docs', []))}")
    print(f"   ‚Ä¢ User context docs: {len(state.get('user_context_docs', []))}")
    print("="*60)
    
    return state

def create_workflow_graph() -> StateGraph:
    """Create and configure the LangGraph workflow"""
    logger.info("üèóÔ∏è Creating workflow graph...")
    
    # Create the state graph
    graph = StateGraph(GraphState)
    
    # Add nodes
    graph.add_node("load_alert", load_alert_node)
    graph.add_node("fetch_call", fetch_call_node)
    graph.add_node("fetch_user_context", fetch_user_context_node)
    graph.add_node("build_prompt", build_prompt_node)
    graph.add_node("llm_analysis", llm_analysis_node)
    graph.add_node("print_output", print_output_node)
    
    # Set entry point
    graph.set_entry_point("load_alert")
    
    # Define the workflow edges
    graph.add_edge("load_alert", "fetch_call")
    graph.add_edge("fetch_call", "fetch_user_context")
    graph.add_edge("fetch_user_context", "build_prompt")
    graph.add_edge("build_prompt", "llm_analysis")
    graph.add_edge("llm_analysis", "print_output")
    graph.add_edge("print_output", END)
    
    logger.info("‚úÖ Workflow graph created successfully")
    return graph

def create_sample_alert_file():
    """Create a simplified sample alert.json file for testing"""
    sample_alert = {
        "call_id": "PLACEHOLDER_ID_FROM_AZURE_SEARCH",
        "user_id": "PLACEHOLDER_DISPLAY_NAME"
    }
    
    with open("alert.json", "w", encoding="utf-8") as f:
        json.dump(sample_alert, f, indent=2)
    
    logger.info("‚úÖ Sample alert.json file created with placeholders")
    logger.info("üìù Please update alert.json with:")
    logger.info("   - call_id: actual 'id' value from your Azure Search CQD index")
    logger.info("   - user_id: actual display name of a participant")

def main():
    """Main execution function"""
    print("üöÄ Starting Microsoft Teams Call Quality Analysis System")
    print("="*60)
    
    try:
        # Validate environment
        if not validate_env_variables():
            print("‚ùå Environment validation failed. Please check your .env file.")
            return False
        
        # Create sample alert file if it doesn't exist
        if not os.path.exists("alert.json"):
            logger.info("üìÑ Creating sample alert.json file...")
            create_sample_alert_file()
        
        # Create and compile the workflow
        graph = create_workflow_graph()
        app = graph.compile()
        
        # Execute the workflow
        logger.info("‚ñ∂Ô∏è Starting analysis workflow...")
        initial_state = {
            "alert": {},
            "call_id": "",
            "user_id": "",
            "call_docs": [],
            "user_context_docs": [],
            "final_prompt": "",
            "insights": {}
        }
        
        final_state = app.invoke(initial_state)
        
        logger.info("‚úÖ Analysis workflow completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error in main execution: {e}")
        print(f"‚ùå System error: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ Analysis completed successfully!")
    else:
        print("\nüí• Analysis failed. Check the logs for details.")
